import os
import jieba
import math
import time
import opencc
import re
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import subprocess
from collections import Counter
from tqdm import tqdm

# è·å–ç³»ç»Ÿé»˜è®¤çš„ä¸­æ–‡å­—ä½“
def get_chinese_font():
    try:
        font_path = subprocess.getoutput("fc-list :lang=zh | head -n 1 | cut -d':' -f1")  # å–ç¬¬ä¸€è¡Œå­—ä½“è·¯å¾„
        return fm.FontProperties(fname=font_path)
    except Exception as e:
        print("æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿå­—ä½“å®‰è£…", e)
        return None
# è®¾ç½®å­—ä½“
font_prop = get_chinese_font()
if font_prop:
    plt.rcParams["font.family"] = font_prop.get_name()
    plt.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# **åˆ›å»º OpenCC ç¹ä½“è½¬ç®€ä½“è½¬æ¢å™¨**
converter = opencc.OpenCC('t2s')


# **åŠ è½½åœç”¨è¯**
def load_stopwords(stopwords_path):
    stopwords = set()
    if os.path.exists(stopwords_path):
        with open(stopwords_path, "r", encoding="utf-8") as f:
            stopwords = set(f.read().splitlines())
    return stopwords


# **ç¹ä½“è½¬ç®€ä½“**
def convert_to_simplified(text):
    print("æ­£åœ¨è¿›è¡Œç¹ä½“è½¬ç®€ä½“...")
    converted_text = []
    chunk_size = max(1, len(text) // 100)  # 100 ä¸ªæ‰¹æ¬¡å¤„ç†ï¼Œé˜²æ­¢æ–‡æœ¬è¿‡é•¿å¡æ­»
    for i in tqdm(range(0, len(text), chunk_size), desc="è½¬æ¢è¿›åº¦"):
        converted_text.append(converter.convert(text[i:i + chunk_size]))
    return "".join(converted_text)

# è¿‡æ»¤éä¸­æ–‡å­—ç¬¦ï¼ˆå»æ‰è‹±æ–‡ã€æ•°å­—ã€ç‰¹æ®Šå­—ç¬¦ï¼‰
def filter_chinese(text):
    return "".join(re.findall(r'[\u4e00-\u9fff]+', text))

# **è®¡ç®— N-Gram ä¿¡æ¯ç†µ**
def calculate_ngram_entropy(text, stopwords, n=1):
    print(f"\nè®¡ç®— {n}-gram ä¿¡æ¯ç†µ...")
    start_time = time.time()
    words = jieba.lcut(text)

    # è¿‡æ»¤åœç”¨è¯
    words = [word for word in tqdm(words, desc="åˆ†è¯è¿›åº¦") if word.strip() and word not in stopwords]

    total_ngrams = len(words) - (n - 1)  # è®¡ç®—æ€»çš„ N-Grams æ•°é‡
    if total_ngrams <= 0:
        print(f"âš ï¸  {n}-gram è¯æ•°è¿‡å°‘ï¼Œæ— æ³•è®¡ç®—ä¿¡æ¯ç†µï¼")
        return 0, 0, 0, Counter()

    # ç”Ÿæˆ N-Grams
    ngram_counts = Counter()
    for i in tqdm(range(total_ngrams), desc=f"ç»Ÿè®¡ {n}-gram é¢‘ç‡"):
        ngram = tuple(words[i:i + n])  # ç”Ÿæˆ N-Gram
        ngram_counts[ngram] += 1

    # è®¡ç®—ä¿¡æ¯ç†µ
    entropy = -sum((count / total_ngrams) * math.log2(count / total_ngrams) for count in ngram_counts.values())
    elapsed_time = time.time() - start_time
    return entropy, total_ngrams, elapsed_time, ngram_counts


# **è¯»å–æ–‡ä»¶å¤¹ a å†…æ‰€æœ‰æ— åç¼€çš„æ–‡æœ¬æ–‡ä»¶**
def load_corpus_from_folder(folder_path):
    corpus_texts = []
    file_list = []

    # éå†æ–‡ä»¶å¤¹åŠå­ç›®å½•
    for root, _, files in os.walk(folder_path):
        for file in files:
            file_path = os.path.join(root, file)
            if os.path.isfile(file_path):  # ç¡®ä¿æ˜¯æ–‡ä»¶ï¼Œè€Œä¸æ˜¯ç›®å½•
                file_list.append(file_path)

    if not file_list:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼")
        return ""

    print(f"ğŸ“‚ æ‰¾åˆ° {len(file_list)} ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œæ­£åœ¨è¯»å–...")

    for file_path in tqdm(file_list, desc="è¯»å–æ–‡ä»¶è¿›åº¦"):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                corpus_texts.append(f.read())
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")

    return "\n".join(corpus_texts)


# **è®¾ç½®è·¯å¾„**
folder_path = "/home/omnisky/lsh/DLNLP/NLPwork/wiki_zh_2019/wiki_zh"  # æ–‡ä»¶å¤¹è·¯å¾„
stopwords_path = "/home/omnisky/lsh/DLNLP/NLPwork/entropy/cn_stopwords.txt"  # åœç”¨è¯æ–‡ä»¶è·¯å¾„

# **åŠ è½½åœç”¨è¯**
stopwords = load_stopwords(stopwords_path)
print(f"âœ… åŠ è½½ {len(stopwords)} ä¸ªåœç”¨è¯")

# **åŠ è½½æ— åç¼€æ–‡ä»¶**
chinese_text = load_corpus_from_folder(folder_path)

if chinese_text:
    # **ç¹ä½“è½¬ç®€ä½“**
    simplified_text = convert_to_simplified(chinese_text)

    # ä»…ä¿ç•™ä¸­æ–‡å­—ç¬¦
    cleaned_text = filter_chinese(simplified_text)

    # **è®¡ç®— 1-gram ä¿¡æ¯ç†µ**
    unigram_entropy, total_unigrams, unigram_time, unigram_counts = calculate_ngram_entropy(cleaned_text, stopwords,
                                                                                            n=1)
    print(f"\nğŸ”¤ 1-gram ä¿¡æ¯ç†µ: {unigram_entropy:.4f}")
    print(f"ğŸ“– 1-gram æ€»æ•°: {total_unigrams}")
    print(f"â³ è®¡ç®—æ—¶é—´: {unigram_time:.4f} ç§’")

    # **è®¡ç®— 2-gram ä¿¡æ¯ç†µ**
    bigram_entropy, total_bigrams, bigram_time, bigram_counts = calculate_ngram_entropy(cleaned_text, stopwords, n=2)
    print(f"\nğŸ”  2-gram ä¿¡æ¯ç†µ: {bigram_entropy:.4f}")
    print(f"ğŸ“– 2-gram æ€»æ•°: {total_bigrams}")
    print(f"â³ è®¡ç®—æ—¶é—´: {bigram_time:.4f} ç§’")

    # **è®¡ç®— 3-gram ä¿¡æ¯ç†µ**
    trigram_entropy, total_trigrams, trigram_time, trigram_counts = calculate_ngram_entropy(cleaned_text, stopwords,
                                                                                            n=3)
    print(f"\nğŸ”¢ 3-gram ä¿¡æ¯ç†µ: {trigram_entropy:.4f}")
    print(f"ğŸ“– 3-gram æ€»æ•°: {total_trigrams}")
    print(f"â³ è®¡ç®—æ—¶é—´: {trigram_time:.4f} ç§’")

    # ä¿®æ­£ 1-gram ç›´æ–¹å›¾
    plt.figure(figsize=(12, 6))
    unigram_freq_sorted = unigram_counts.most_common(50)
    x_unigrams = [''.join(word[0]) for word in unigram_freq_sorted]  # ç¡®ä¿ x è½´æ˜¯å­—ç¬¦ä¸²
    y_unigrams = [word[1] for word in unigram_freq_sorted]
    plt.bar(x_unigrams, y_unigrams, color='blue')
    plt.title("Chinese 1-Gram Frequency Distribution (Top 50)", fontproperties=font_prop)
    plt.xlabel("Character")
    plt.ylabel("Occurrences")
    plt.show()

    # ä¿®æ­£ 2-gram ç›´æ–¹å›¾
    plt.figure(figsize=(12, 6))
    bigram_freq_sorted = bigram_counts.most_common(50)
    x_bigrams = [''.join(word[0]) for word in bigram_freq_sorted]  # ç¡®ä¿ x è½´æ˜¯å­—ç¬¦ä¸²
    y_bigrams = [word[1] for word in bigram_freq_sorted]
    plt.bar(x_bigrams, y_bigrams, color='red')
    plt.title("Chinese 2-Gram Frequency Distribution (Top 50)", fontproperties=font_prop)
    plt.xlabel("Bigram")
    plt.ylabel("Occurrences")
    plt.xticks(rotation=45)
    plt.show()

    # ä¿®æ­£ 3-gram ç›´æ–¹å›¾
    plt.figure(figsize=(12, 6))
    trigram_freq_sorted = trigram_counts.most_common(50)
    x_trigrams = [''.join(word[0]) for word in trigram_freq_sorted]  # ç¡®ä¿ x è½´æ˜¯å­—ç¬¦ä¸²
    y_trigrams = [word[1] for word in trigram_freq_sorted]
    plt.bar(x_trigrams, y_trigrams, color='green')
    plt.title("Chinese 3-Gram Frequency Distribution (Top 50)", fontproperties=font_prop)
    plt.xlabel("Trigram")
    plt.ylabel("Occurrences")
    plt.xticks(rotation=45)
    plt.show()


