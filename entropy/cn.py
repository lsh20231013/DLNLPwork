import os
import jieba
import math
import time
import opencc
import re
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import subprocess
from collections import Counter
from tqdm import tqdm

# è·å–ç³»ç»Ÿé»˜è®¤çš„ä¸­æ–‡å­—ä½“
def get_chinese_font():
    try:
        font_path = subprocess.getoutput("fc-list :lang=zh | head -n 1 | cut -d':' -f1")  # å–ç¬¬ä¸€è¡Œå­—ä½“è·¯å¾„
        return fm.FontProperties(fname=font_path)
    except Exception as e:
        print("æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œè¯·æ£€æŸ¥ç³»ç»Ÿå­—ä½“å®‰è£…", e)
        return None
# è®¾ç½®å­—ä½“
font_prop = get_chinese_font()
if font_prop:
    plt.rcParams["font.family"] = font_prop.get_name()
    plt.rcParams["axes.unicode_minus"] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

# åˆ›å»º OpenCC è½¬æ¢å™¨ï¼ˆç¹ä½“ -> ç®€ä½“ï¼‰
converter = opencc.OpenCC('t2s')


# åŠ è½½åœç”¨è¯
def load_stopwords(stopwords_path):
    if os.path.exists(stopwords_path):
        with open(stopwords_path, "r", encoding="utf-8") as f:
            return set(f.read().splitlines())
    return set()


# ç¹ä½“è½¬ç®€ä½“ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
def convert_to_simplified(text):
    print("æ­£åœ¨è¿›è¡Œç¹ä½“è½¬ç®€ä½“...")
    chunk_size = max(1, len(text) // 100)
    return "".join(
        converter.convert(text[i:i + chunk_size]) for i in tqdm(range(0, len(text), chunk_size), desc="è½¬æ¢è¿›åº¦"))


# è¿‡æ»¤éä¸­æ–‡å­—ç¬¦ï¼ˆå»æ‰è‹±æ–‡ã€æ•°å­—ã€ç‰¹æ®Šå­—ç¬¦ï¼‰
def filter_chinese(text):
    return "".join(re.findall(r'[\u4e00-\u9fff]+', text))


# è®¡ç®—å­—ç¬¦çº§ä¿¡æ¯ç†µ
def calculate_character_entropy(text):
    print("\nè®¡ç®—å­—ç¬¦çº§ä¿¡æ¯ç†µ...")
    start_time = time.time()
    char_counts = Counter(text)
    total_chars = sum(char_counts.values())

    entropy = -sum((count / total_chars) * math.log2(count / total_chars) for count in char_counts.values())
    return entropy, total_chars, time.time() - start_time, char_counts


# è®¡ç®—è¯çº§ä¿¡æ¯ç†µ
def calculate_word_entropy(text, stopwords):
    print("\nè®¡ç®—è¯çº§ä¿¡æ¯ç†µ...")
    start_time = time.time()
    words = [word for word in jieba.lcut(text) if word.strip() and word not in stopwords]
    word_counts = Counter(words)
    total_words = sum(word_counts.values())

    entropy = -sum((count / total_words) * math.log2(count / total_words) for count in word_counts.values())
    return entropy, total_words, time.time() - start_time, word_counts


# è¯»å–æ–‡ä»¶å¤¹ a å†…æ‰€æœ‰æ— åç¼€çš„æ–‡æœ¬æ–‡ä»¶
def load_corpus_from_folder(folder_path):
    corpus_texts = []
    file_list = [os.path.join(root, file) for root, _, files in os.walk(folder_path) for file in files]

    if not file_list:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æœ¬æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œç›®å½•ç»“æ„ï¼")
        return ""

    print(f"ğŸ“‚ æ‰¾åˆ° {len(file_list)} ä¸ªæ–‡æœ¬æ–‡ä»¶ï¼Œæ­£åœ¨è¯»å–...")

    for file_path in tqdm(file_list, desc="è¯»å–æ–‡ä»¶è¿›åº¦"):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                corpus_texts.append(f.read())
        except Exception as e:
            print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {e}")

    return "\n".join(corpus_texts)


# è®¾ç½®è¯­æ–™åº“æ–‡ä»¶å¤¹è·¯å¾„
folder_path = "/home/omnisky/lsh/DLNLP/NLPwork/wiki_zh_2019/wiki_zh"  # æ–‡ä»¶å¤¹è·¯å¾„
stopwords_path = "/home/omnisky/lsh/DLNLP/NLPwork/entropy/cn_stopwords.txt"  # åœç”¨è¯æ–‡ä»¶è·¯å¾„

# åŠ è½½åœç”¨è¯
stopwords = load_stopwords(stopwords_path)
print(f"âœ… åŠ è½½ {len(stopwords)} ä¸ªåœç”¨è¯")

# åŠ è½½æ‰€æœ‰æ— åç¼€æ–‡ä»¶
chinese_text = load_corpus_from_folder(folder_path)

if chinese_text:
    # ç¹ä½“è½¬ç®€ä½“
    simplified_text = convert_to_simplified(chinese_text)

    # ä»…ä¿ç•™ä¸­æ–‡å­—ç¬¦
    cleaned_text = filter_chinese(simplified_text)

    # è®¡ç®—å­—ç¬¦çº§ä¿¡æ¯ç†µ
    char_entropy, total_chars, char_time, char_counts = calculate_character_entropy(cleaned_text)
    print(f"\nğŸ”¤ ä¸­æ–‡å­—ç¬¦çº§ä¿¡æ¯ç†µ: {char_entropy:.4f}")
    print(f"ğŸ“– è¯­æ–™åº“æ€»å­—ç¬¦æ•°: {total_chars}")
    print(f"â³ å­—ç¬¦ç†µè®¡ç®—æ—¶é—´: {char_time:.4f} ç§’")

    # è®¡ç®—è¯çº§ä¿¡æ¯ç†µ
    word_entropy, total_words, word_time, word_counts = calculate_word_entropy(cleaned_text, stopwords)
    print(f"\nğŸ“ ä¸­æ–‡è¯çº§ä¿¡æ¯ç†µ: {word_entropy:.4f}")
    print(f"ğŸ“– è¯­æ–™åº“æ€»å•è¯æ•°ï¼ˆå»åœç”¨è¯ï¼‰: {total_words}")
    print(f"â³ è¯ç†µè®¡ç®—æ—¶é—´: {word_time:.4f} ç§’")

    # ç»˜åˆ¶å­—ç¬¦é¢‘ç‡ç›´æ–¹å›¾
    plt.figure(figsize=(12, 6))
    char_freq_sorted = char_counts.most_common(50)
    plt.bar([char[0] for char in char_freq_sorted], [char[1] for char in char_freq_sorted], color='blue')
    plt.title("Chinese Character Frequency Distribution (Top 50)", fontproperties=font_prop)
    plt.xlabel("Character")
    plt.ylabel("Occurrences")
    plt.show()

    # ç»˜åˆ¶è¯é¢‘ç›´æ–¹å›¾
    plt.figure(figsize=(12, 6))
    word_freq_sorted = word_counts.most_common(50)
    plt.bar([word[0] for word in word_freq_sorted], [word[1] for word in word_freq_sorted], color='green')
    plt.title("Chinese Word Frequency Distribution (Top 50)", fontproperties=font_prop)
    plt.xlabel("Word")
    plt.ylabel("Occurrences")
    plt.xticks(rotation=45)
    plt.show()